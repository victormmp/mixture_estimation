{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exercício 5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Victor Marcius Magalhães Pinto"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Descrição do Problema"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Considerando dados sintéticos, gerados para problemas não-linearmente separáveis, e.g. espirais, utilize o classificador Bayesiano para resolver os problemas de classificação correspondentes, utilizando mistura de Gaussianas com clustering e o KDE estimado pelo método do Silverman. Aplicar os dois métodos para o problema do Golub e comparar os resultados com aqueles obtidos considerando uma única Gaussiana por classe. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testando métodos para dados sintéticos"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Gerando dados"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inicialmente vamos preparar o ambiente do notebook utilizado para os experimentos deste exercício. Os pacotes necessários serão importados a seguir."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_validate, ShuffleSplit\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, recall_score, precision_score, roc_curve\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler, normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "from os.path import join\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from IPython.core.debugger import set_trace\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Primeiro vamos implementar a função geradora de dados em espiral, a ser utilizada como testbench para o classificador por mistura de gaussianas. Agradecimentos aqui ao Henrico Barbosa pela sugestão de função geradora."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def twospirals(n_points, n_turns, ts=np.pi, tinc=1, noise=.3):\n",
    "    \"\"\"\n",
    "     Returns the two spirals dataset.\n",
    "     modificado de: \n",
    "         https://glowingpython.blogspot.com/2017/04/solving-two-spirals-problem-with-keras.html\n",
    "    Primeiro gera uma espiral e obtem a segunda espelhando a primeira\n",
    "    \"\"\"\n",
    "    # equação da espiral (coord polares): r = tinc*theta\n",
    "    # n_points: número de pontos de cada espiral\n",
    "    # n_turns: número de voltas das espirais\n",
    "    # ts: ângulo inicial da espiral em radianos\n",
    "    # tinc: taxa de crescimento do raio em função do ângulo\n",
    "    # noise: desvio-padrão do ruído\n",
    "    \n",
    "    # Sorteando aleatoriamente pontos da espiral\n",
    "    n = np.sqrt(np.random.rand(n_points,1))  #intervalo [0,1] equivale a [0,theta_max]\n",
    "                                             #tomar a raiz quadrada ajuda a \n",
    "                                             #distribuir melhor os pontos\n",
    "    ns = (ts)/(2*np.pi*n_turns) #ponto do intervalo equivalente a ts radianos\n",
    "    n = ns + n_turns*n # intervalo [ns,ns+n_turns] equivalente a [ts, theta_max]\n",
    "    n = n*(2*np.pi) #intervalo [ts, theta_max]\n",
    "    \n",
    "    # Espiral 1\n",
    "    d1x = np.cos(n)*tinc*n + np.random.randn(n_points,1) * noise\n",
    "    d1y = np.sin(n)*tinc*n + np.random.randn(n_points,1) * noise\n",
    "    \n",
    "    # Espiral 2\n",
    "    d2x = -np.cos(n)*tinc*n + np.random.randn(n_points,1) * noise\n",
    "    d2y = -np.sin(n)*tinc*n + np.random.randn(n_points,1) * noise\n",
    "    \n",
    "    spirals_points = np.vstack((np.hstack((d1x,d1y)),np.hstack((d2x,d2y))))\n",
    "    points_labels = np.hstack((np.ones(n_points),np.zeros(n_points)))\n",
    "    return (spirals_points, points_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Geramos a seguir os dados utilizados para treinamento do classificador, que consiste na obtenção dos clusters e nas estatísticas associadas a cada um deles. Tembém geramos os dados de teste."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x, y = twospirals(800,2,ts=np.pi,tinc=1,noise=0.5)\n",
    "x_test, y_test = twospirals(1000,2,ts=np.pi,tinc=1,noise=0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "a distribuição dos dados de treinamento:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('Data Set Treinamento')\n",
    "plt.plot(x[y==0,0], x[y==0,1], '.', label='Classe 1')\n",
    "plt.plot(x[y==1,0], x[y==1,1], '.', label='Classe 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "e a distribuição dos dados de teste:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('Data Set Teste')\n",
    "plt.plot(x_test[y_test==0,0], x_test[y_test==0,1], '.', label='Classe 1')\n",
    "plt.plot(x_test[y_test==1,0], x_test[y_test==1,1], '.', label='Classe 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Implementamos agora o classificador que faz uso de mistura de gaussianas para a separação de classes não-linearmente separáveis, como as dos conjuntos obtidos acima. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class GaussianMixture:\n",
    "    \n",
    "    def __init__(self, clusters=(3, 3)):\n",
    "        \"\"\"\n",
    "        Gaussian mixture Model.\n",
    "        Args:\n",
    "            custers: number of clusters to use for each class. Considering binary problems, \n",
    "                it is a list (or tuple) with two int values.\n",
    "        \"\"\"\n",
    "        self.clusters = clusters\n",
    "    \n",
    "    def calc_statistics(self, x):\n",
    "        \"\"\"\n",
    "        Calculate mean and covariance matrix for a distribution.\n",
    "        Args:\n",
    "            x: data distribution.\n",
    "        Returns:\n",
    "            Mean and covariance matrix for current distribution.\n",
    "        \"\"\"\n",
    "        mean_1 = np.mean(x, axis=0)\n",
    "        sigma_1 = np.cov(x.T)\n",
    "        \n",
    "        return mean_1, sigma_1\n",
    "    \n",
    "    def get_statistics(self, x, k):\n",
    "        \"\"\"\n",
    "        Get statistics for gaussian mixture for a distribution.\n",
    "        Args:\n",
    "            x: Data distribution.\n",
    "            k: number of clusters to split data.\n",
    "        Returns:\n",
    "            A list of dictionaries, for each of the k clusters, with its mean, covariance matrix and\n",
    "            cluster probability.\n",
    "        \"\"\"\n",
    "        kmeans = KMeans(n_clusters=k)\n",
    "        kmeans.fit(x)\n",
    "        N = x.size\n",
    "        cluster_stat = []\n",
    "        for cluster in range(k):\n",
    "            c_samples = kmeans.labels_ == cluster\n",
    "            N_k = x[c_samples].shape[0]\n",
    "            pi_k = N_k / N\n",
    "            mean, sigma = self.calc_statistics(x[c_samples])\n",
    "            cluster_stat.append(\n",
    "                {\n",
    "                    'mean': mean, \n",
    "                    'sigma': sigma, \n",
    "                    'pi_k': pi_k\n",
    "                }\n",
    "            )\n",
    "        return cluster_stat\n",
    "    \n",
    "    def fit(self, x_train, y_train):\n",
    "        \"\"\"\n",
    "        Fit the gaussian mixture model.\n",
    "        Args:\n",
    "            x_train: Training samples.\n",
    "            y_train: Training samples labels.\n",
    "        \"\"\"\n",
    "        labels = list(Counter(y_train).keys())\n",
    "        self.labels = {'class_1': labels[0], 'class_2': labels[1]}\n",
    "        \n",
    "        p_class_1 = x_train[y_train == labels[0], :].shape[0]\n",
    "        p_class_2 = x_train[y_train == labels[1], :].shape[0]\n",
    "\n",
    "        self.k = p_class_1 / p_class_2\n",
    "        \n",
    "        self.statistics_c1 = self.get_statistics(x_train[y_train == labels[0], :], self.clusters[0])\n",
    "        self.statistics_c2 = self.get_statistics(x_train[y_train == labels[1], :], self.clusters[1])\n",
    "\n",
    "    def likelihood(self, x, mean, sigma):\n",
    "        \"\"\"\n",
    "        Calculate likelihood of a sample with a gaussian distribution.\n",
    "        Args:\n",
    "            x: Sample.\n",
    "            mean: Mean of the gaussian distribution.\n",
    "            sigma: Covariance matrixof the gaussian distribution.\n",
    "        Returns:\n",
    "            The likehood value.\n",
    "        \"\"\"\n",
    "        n = mean.shape[0]\n",
    "        arg = (x - mean)\n",
    "        term = np.matmul(np.matmul(arg.T, np.linalg.pinv(sigma)), arg)\n",
    "        return 1/(np.sqrt(((2*np.pi)**n)*np.linalg.det(sigma))) *np.exp(-0.5*term)\n",
    "    \n",
    "    def clusters_likelihood(self, x, statistics):\n",
    "        \"\"\"\n",
    "        Calculate the likelihood of a sample with a gaussian mixture distribution.\n",
    "        Args:\n",
    "            x: Sample.\n",
    "            statistics: List of dictionaries containing mean, the covariance matrix and the\n",
    "                cluster probability for each cluster of the gaussian mixture.\n",
    "        Returns:\n",
    "            The gaussian mixture likelihood of the sample.\n",
    "        \"\"\"\n",
    "        p_x_c = []\n",
    "        for c in statistics:\n",
    "            p_x_c.append(\n",
    "                c['pi_k'] * self.likelihood(x, c['mean'], c['sigma'])\n",
    "            )\n",
    "        return np.sum(p_x_c)\n",
    "\n",
    "    def bayes_decision_index(self, x):\n",
    "        \"\"\"\n",
    "        Calculate the bayes index as a ratio between the likelihoods of a sample with the two classes\n",
    "        of the system.\n",
    "        Args:\n",
    "            x: The Sample.\n",
    "        Returns:\n",
    "            The bayes index.\n",
    "        \"\"\"\n",
    "        return self.clusters_likelihood(x, self.statistics_c1) / self.clusters_likelihood(x, self.statistics_c2)\n",
    "\n",
    "    def gaussian_classifier(self, x):\n",
    "        \"\"\"\n",
    "        Classifies a sample according to its bayes index.\n",
    "        Args:\n",
    "            x: Sample.\n",
    "        Returns:\n",
    "            The predicted class of the sample.\n",
    "        \"\"\"\n",
    "        return self.labels['class_1'] if self.bayes_decision_index(x) >= self.k else self.labels['class_2']\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Batch predict of the gaussian mixture model classifier.\n",
    "        Args:\n",
    "            x: Sample matrix.\n",
    "        Returns:\n",
    "            Predictions array for each sample.\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for sample in tqdm(x):\n",
    "            predictions.append(self.gaussian_classifier(sample))\n",
    "        return predictions\n",
    "    \n",
    "    def predict_indexes(self, x):\n",
    "        predictions = []\n",
    "        for sample in tqdm(x):\n",
    "            predictions.append(self.bayes_decision_index(sample))\n",
    "        return predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Alguns experimentos interessantes a serem observados com os dados utilizados, e o classificador por mistura de gaussianas. Vamos variar a quantidade de clusters utilizados para segmentar as classes e acompanhar a performance do modelo, particularmente o F1-score."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scores = []\n",
    "for k in range(1, 21):\n",
    "    gm = GaussianMixture(clusters=(k, k))\n",
    "    gm.fit(x, y)\n",
    "    pred = gm.predict(x_test)\n",
    "    scores.append(\n",
    "        (\n",
    "            k, \n",
    "            f1_score(y_test, pred, average='weighted'), \n",
    "            recall_score(y_test, pred, average='weighted'), \n",
    "            precision_score(y_test, pred, average='weighted')\n",
    "        )\n",
    "    )\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scores = np.array(scores)\n",
    "plt.figure()\n",
    "plt.title('Pontuações de performance do classificador')\n",
    "plt.ylabel('F1-score')\n",
    "plt.xlabel('Número de clusters na mistura')\n",
    "plt.plot(scores[:,0], scores[:,1], 'o-', label='F1-score')\n",
    "plt.plot(scores[:,0], scores[:,3], 'x-', label='Precision')\n",
    "plt.plot(scores[:,0], scores[:,2], '.-', label='Recall')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "É interessante notar como, à medida que aumentamos a quantidade de clusters utilizados pelo k-means para descrever os dados, em cada classe, aumentamos o score de classificação  dos dados. Isto é esperado, uma vez que, para uma quantidade maior de clusters, a discriminação dos dados em cada classe torna-se melhor, como esperado do método de mistura de gaussianas. Nota-se, também como, a partir de um determinado número de clusters utilizados, o score não apresenta uma evolução notável, uma vez que segmentar mais os dados em números maiores de agrupamentos não adiciona discriminibilidade às classes, e portanto não adiciona ganho em performance.\n",
    "\n",
    "Adotando-se, portanto, um número de 10 clusters para os dados sintéticos, temos o seguinte desempenho da rede implementada."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gm = GaussianMixture(clusters=(10, 10))\n",
    "gm.fit(x, y)\n",
    "\n",
    "pred = gm.predict(x_test)\n",
    "\n",
    "print(classification_report(y_test, pred))\n",
    "fpr, tpr, _ = roc_curve(y_test, pred)\n",
    "plt.figure() \n",
    "plt.title('ROC')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.plot(fpr, tpr)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "e a superfície de separação"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grid_size=200\n",
    "grid_range=(-15, 15)\n",
    "x_lab = np.linspace(grid_range[0], grid_range[1], num=grid_size)\n",
    "y_lab = np.linspace(grid_range[0], grid_range[1], num=grid_size)\n",
    "x1, x2 = np.meshgrid(x_lab, y_lab)\n",
    "x_grid = np.transpose(np.vstack([x1.flatten(), x2.flatten()]))\n",
    "\n",
    "y_hat = np.array(gm.predict_indexes(x_grid))\n",
    "y_hat = y_hat.reshape([grid_size,grid_size])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(x_test[y_test==0,0], x_test[y_test==0,1], '.', label='Classe 1')\n",
    "plt.plot(x_test[y_test==1,0], x_test[y_test==1,1], '.', label='Classe 2')\n",
    "plt.contour(x1, x2, y_hat, levels=[1], colors=('black',), linewidths=(2.5,))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Implementando funções de Estimador de Densidade de Kernel (KDE):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class KDE:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.labels = {}\n",
    "        self.X_1 = None\n",
    "        self.X_2 = None\n",
    "        self.h = None\n",
    "        self.k = None\n",
    "    \n",
    "    def kernel(self, x):\n",
    "        return 1 / np.sqrt(2*np.pi) * np.exp(-0.5 * x*x)\n",
    "    \n",
    "    def fit(self, x_train, y_train, std=None):\n",
    "        labels = list(Counter(y_train).keys())\n",
    "        self.X_1 = x_train[y_train == labels[0]]\n",
    "        self.X_2 = x_train[y_train == labels[1]]\n",
    "        \n",
    "        self.labels = {'class_1': labels[0], 'class_2': labels[1]}\n",
    "        \n",
    "        p_class_1 = x_train[y_train == labels[0], :].shape[0]\n",
    "        p_class_2 = x_train[y_train == labels[1], :].shape[0]\n",
    "\n",
    "        self.k = p_class_1 / p_class_2\n",
    "        \n",
    "        self.h_1 = self.estimate_h(self.X_1, std)\n",
    "        self.h_2 = self.estimate_h(self.X_2, std)\n",
    "    \n",
    "    def density_estimation(self, x, h, data):\n",
    "        kernel = [np.prod(1/h * self.kernel((x-sample_x)/h)) for sample_x in data]\n",
    "        return np.sum(kernel) / len(kernel)\n",
    "    \n",
    "    def estimate_h(self, data, std):\n",
    "        if std:\n",
    "            return np.linalg.norm(1.06 * std * np.power(data.shape[0], -0.2))\n",
    "        return np.linalg.norm(1.06 * np.std(data, axis=0) * np.power(data.shape[0], -0.2))\n",
    "        \n",
    "    \n",
    "    def bayes_decision_index(self, x):\n",
    "        \"\"\"\n",
    "        Calculate the bayes index as a ratio between the likelihoods of a sample with the two classes\n",
    "        of the system.\n",
    "        Args:\n",
    "            x: The Sample.\n",
    "        Returns:\n",
    "            The bayes index.\n",
    "        \"\"\"\n",
    "        return self.density_estimation(x, self.h_1, self.X_1) / self.density_estimation(x, self.h_2, self.X_2)\n",
    "\n",
    "    def gaussian_classifier(self, x):\n",
    "        \"\"\"\n",
    "        Classifies a sample according to its bayes index.\n",
    "        Args:\n",
    "            x: Sample.\n",
    "        Returns:\n",
    "            The predicted class of the sample.\n",
    "        \"\"\"\n",
    "        return self.labels['class_1'] if self.bayes_decision_index(x) >= self.k else self.labels['class_2']\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Batch predict of the gaussian mixture model classifier.\n",
    "        Args:\n",
    "            x: Sample matrix.\n",
    "        Returns:\n",
    "            Predictions array for each sample.\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for sample in tqdm(x):\n",
    "            predictions.append(self.gaussian_classifier(sample))\n",
    "        return predictions\n",
    "    \n",
    "    def predict_indexes(self, x):\n",
    "        predictions = []\n",
    "        for sample in tqdm(x):\n",
    "            predictions.append(self.bayes_decision_index(sample))\n",
    "        return predictions\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kde = KDE()\n",
    "kde.fit(x, y, 0.5)\n",
    "\n",
    "pred = kde.predict(x_test)\n",
    "\n",
    "print(classification_report(y_test, pred))\n",
    "fpr, tpr, _ = roc_curve(y_test, pred)\n",
    "plt.figure() \n",
    "plt.title('ROC')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.plot(fpr, tpr)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grid_size=100\n",
    "grid_range=(-15, 15)\n",
    "x_lab = np.linspace(grid_range[0], grid_range[1], num=grid_size)\n",
    "y_lab = np.linspace(grid_range[0], grid_range[1], num=grid_size)\n",
    "x1, x2 = np.meshgrid(x_lab, y_lab)\n",
    "x_grid = np.transpose(np.vstack([x1.flatten(), x2.flatten()]))\n",
    "\n",
    "y_hat = np.array(kde.predict_indexes(x_grid))\n",
    "y_hat = y_hat.reshape([grid_size,grid_size])\n",
    "\n",
    "plt.plot(x_test[y_test==0,0], x_test[y_test==0,1], '.', label='Classe 1')\n",
    "plt.plot(x_test[y_test==1,0], x_test[y_test==1,1], '.', label='Classe 2')\n",
    "plt.contour(x1, x2, y_hat, levels=[1], colors=('black',), linewidths=(2.5,))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testando métodos para o dataset de expressão gênica"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Houve problemas na aplicação das soluções de classificação no dataset disponível de expressão gênica."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Carregando os dados"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def clean_dataset(data, normalized=False, scaler=None):\n",
    "    df = data.copy()\n",
    "    columns = [c for c in df.columns if 'call' in c]\n",
    "    df = df.set_index('Gene Accession Number')\n",
    "    df.index.name = None\n",
    "    df = df.drop(columns + ['Gene Description'], axis=1).transpose()\n",
    "    df.index = df.index.astype(int)\n",
    "    \n",
    "    if not normalized:\n",
    "        return df, None\n",
    "    \n",
    "    x = df.values\n",
    "    \n",
    "    if scaler is not None:\n",
    "        minmax = scaler\n",
    "    else:\n",
    "        minmax = MinMaxScaler()\n",
    "        minmax.fit(x)\n",
    "        \n",
    "    x_scaled = minmax.transform(x)\n",
    "    df.loc[:,:] = x_scaled\n",
    "    return df, minmax\n",
    "\n",
    "path = os.path.join('.', 'data_set_ALL_AML_train.csv')\n",
    "train, scaler = clean_dataset(pd.read_csv(path))\n",
    "\n",
    "path = os.path.join('.', 'data_set_ALL_AML_independent.csv')\n",
    "test, _ = clean_dataset(pd.read_csv(path), scaler=scaler)\n",
    "\n",
    "df = pd.concat([train, test], copy=False)\n",
    "\n",
    "path = os.path.join('.', 'actual.csv')\n",
    "cf_label = pd.read_csv(path)\n",
    "cf_label = cf_label.set_index('patient')\n",
    "cf_label.index.name = None\n",
    "\n",
    "df = df.join(cf_label)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df.drop('cancer', axis=1), df.cancer, test_size=0.4)\n",
    "\n",
    "mean_1 = x_train.loc[y_train == 'AML', :].mean().to_numpy()\n",
    "mean_2 = x_train.loc[y_train == 'ALL', :].mean().to_numpy()\n",
    "\n",
    "std_1 = x_train.loc[y_train == 'AML', :].std().to_numpy()\n",
    "std_2 = x_train.loc[y_train == 'ALL', :].std().to_numpy()\n",
    "\n",
    "P = np.abs((mean_1 - mean_2)) / (std_1 + std_2)\n",
    "features = np.argsort(P)[-50:]\n",
    "\n",
    "x_train = x_train.iloc[:, features].to_numpy()\n",
    "x_test = x_test.iloc[:, features].to_numpy()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Relizando uma análise dos dados, um primeiro problema é a quantidade de dados disponíveis para cada amostra. No conjunto de treinamento, temos as seguintes configurações de amostras para cada classe:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Quantidade de pacientes para a classe ALL: ', x_train[y_train == 'AML', :].shape[0])\n",
    "print('Quantidade de pacientes para a classe AML: ', x_train[y_train == 'ALL', :].shape[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para o caso do classificador com mistura de gaussianas, isto já limita a quantidade de clusters que podemos considerar para a segmentação das classes, uma vez que uma alta quantidade de clusters pode acabar por gerar centróides duplicados ou clusters com matriz de covariância unitária (tendo emvista clusters com apenas uma única amostra). Além disso, ao avaliarmos o determinante da matriz de covariância entre as amostras das classes, utilizada no cálculo da gaussiana multivariável:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sigma_1 = np.cov(x_train[y_train == 'AML', :].T)\n",
    "sigma_2 = np.cov(x_train[y_train == 'ALL', :].T)\n",
    "print(f'Determinante matriz de covariâncias da classe 1 (ALL) = {np.linalg.det(sigma_1)}')\n",
    "print(f'Determinante matriz de covariâncias da classe 2 (AML) = {np.linalg.det(sigma_2)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Um determinante nulo implica, além de uma matriz não invertível, um  problema no cálculo da verossimilhança, onde o determinante da matriz de covariâncias aparece como denominador. Com isso, a verossimilhança de uma amostra para esta classe tende a infinito, e portanto, para qualquer amostra prevista, seu resultado será esta classe. Isto será visto nos testes a seguir."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gm_ge = GaussianMixture(clusters=(2, 2))\n",
    "gm_ge.fit(x_train, y_train)\n",
    "\n",
    "pred = gm_ge.predict(x_test)\n",
    "\n",
    "print(classification_report(y_test, pred))\n",
    "y_test_1 = [1 if s == 'AML' else 0 for s in y_test]\n",
    "pred_1 = [1 if s == 'AML' else 0 for s in pred]\n",
    "fpr, tpr, _ = roc_curve(y_test_1, pred_1)\n",
    "plt.figure() \n",
    "plt.title('ROC')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.plot(fpr, tpr)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kde = KDE()\n",
    "kde.fit(x_train, y_train)\n",
    "\n",
    "pred = kde.predict(x_test)\n",
    "\n",
    "print(classification_report(y_test, pred))\n",
    "y_test_1 = [1 if s == 'AML' else 0 for s in y_test]\n",
    "pred_1 = [1 if s == 'AML' else 0 for s in pred]\n",
    "fpr, tpr, _ = roc_curve(y_test_1, pred_1)\n",
    "plt.figure() \n",
    "plt.title('ROC')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.plot(fpr, tpr)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Como visto das predições acima, tanto para mistura de gaussianas quanto para KDE, os desempenhos não foram satisfatórios. Ambos previram apenas amostras para a classe ALL, como comentado anteriormente. Se observarmos a curva ROC, vemos que seu desempenho é similar ao de um classificador aleatório. \n",
    "\n",
    "Este desempenho se deu analisando as 50 melhores variáveis considerando o critério de correlação que o paper correspondente aplicou. Vamos fazer um pouco diferente agora, e vamos fazer uso de PCA para selecionar duas melhores componentes considerando todo o conjunto de dados. O modelo do PCA será treinado utilizando o conjunto de treinamento, e então aplicado ao conjunto de teste, para evitar a inserção de viés nos dados testados."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path = os.path.join('.', 'data_set_ALL_AML_train.csv')\n",
    "train, scaler = clean_dataset(pd.read_csv(path), normalized=False)\n",
    "\n",
    "path = os.path.join('.', 'data_set_ALL_AML_independent.csv')\n",
    "test, _ = clean_dataset(pd.read_csv(path), scaler=scaler, normalized=False)\n",
    "\n",
    "df = pd.concat([train, test], copy=False)\n",
    "\n",
    "path = os.path.join('.', 'actual.csv')\n",
    "cf_label = pd.read_csv(path)\n",
    "cf_label = cf_label.set_index('patient')\n",
    "cf_label.index.name = None\n",
    "\n",
    "df = df.join(cf_label)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df.drop('cancer', axis=1), df.cancer, test_size=0.4)\n",
    "x_train = pca.fit_transform(x_train)\n",
    "x_test = pca.transform(x_test)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sigma_1 = np.cov(x_train[y_train == 'AML', :].T)\n",
    "sigma_2 = np.cov(x_train[y_train == 'ALL', :].T)\n",
    "print(f'Determinante matriz de covariâncias da classe 1 (ALL) = {np.linalg.det(sigma_1)}')\n",
    "print(f'Determinante matriz de covariâncias da classe 2 (AML) = {np.linalg.det(sigma_2)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "De maneira similar ao que fizemos anteriormente para o conjunto de dados sintéticos, vamos avaliar o desempenho do classificador para diferentes números de clusters utilizados na mistura de gaussianas, por classe."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scores = []\n",
    "for k in range(1, 11):\n",
    "    gm = GaussianMixture(clusters=(k, k))\n",
    "    gm.fit(x_train, y_train)\n",
    "    pred = gm.predict(x_test)\n",
    "    scores.append(\n",
    "        (\n",
    "            k, \n",
    "            f1_score(y_test, pred, average='weighted'), \n",
    "            recall_score(y_test, pred, average='weighted'), \n",
    "            precision_score(y_test, pred, average='weighted')\n",
    "        )\n",
    "    )\n",
    "    \n",
    "scores = np.array(scores)\n",
    "plt.figure()\n",
    "plt.title('Pontuações de performance do classificador')\n",
    "plt.ylabel('F1-score')\n",
    "plt.xlabel('Número de clusters na mistura')\n",
    "plt.plot(scores[:,0], scores[:,1], 'o-', label='F1-score')\n",
    "plt.plot(scores[:,0], scores[:,3], 'x-', label='Precision')\n",
    "plt.plot(scores[:,0], scores[:,2], '.-', label='Recall')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Utilizando portanto 2 clusters por classe para dividir os dados, temos."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gm_ge = GaussianMixture(clusters=(2, 2))\n",
    "gm_ge.fit(x_train, y_train)\n",
    "\n",
    "pred = gm_ge.predict(x_test)\n",
    "\n",
    "print(classification_report(y_test, pred))\n",
    "y_test_1 = np.array([1 if s == 'AML' else 0 for s in y_test])\n",
    "pred_1 = np.array([1 if s == 'AML' else 0 for s in pred])\n",
    "fpr, tpr, _ = roc_curve(y_test_1, pred_1)\n",
    "plt.figure() \n",
    "plt.title('ROC')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.plot(fpr, tpr)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A superfície de separação, e os dados de teste classficados de acordo podem ser vistos a seguir."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grid_size=200\n",
    "grid_range=(-100000, 100000)\n",
    "x_lab = np.linspace(grid_range[0], grid_range[1], num=grid_size)\n",
    "y_lab = np.linspace(grid_range[0], grid_range[1], num=grid_size)\n",
    "x1, x2 = np.meshgrid(x_lab, y_lab)\n",
    "x_grid = np.transpose(np.vstack([x1.flatten(), x2.flatten()]))\n",
    "\n",
    "y_hat = np.array(gm_ge.predict_indexes(x_grid))\n",
    "y_hat = y_hat.reshape([grid_size,grid_size])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(x_test[y_test_1==0,0], x_test[y_test_1==0,1], '.', label='Classe 1')\n",
    "plt.plot(x_test[y_test_1==1,0], x_test[y_test_1==1,1], '.', label='Classe 2')\n",
    "plt.contour(x1, x2, y_hat, levels=[1], colors=('black',), linewidths=(2.5,))\n",
    "plt.title('Superfície de separação e dados de teste')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Os dados de treinamento utilizados para a obtenção da superfície podem ser vistos juntamente com a mesma nos gráficos a seguir."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "y_train_1 = np.array([1 if s == 'AML' else 0 for s in y_train])\n",
    "plt.plot(x_train[y_train_1==0,0], x_train[y_train_1==0,1], '.', label='Classe 1')\n",
    "plt.plot(x_train[y_train_1==1,0], x_train[y_train_1==1,1], '.', label='Classe 2')\n",
    "plt.contour(x1, x2, y_hat, levels=[1], colors=('black',), linewidths=(2.5,))\n",
    "plt.title('Superfície de separação e dados de treinamento')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "No caso do modelo KDE, o desempenho de treinamento é:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kde_ge = KDE()\n",
    "kde_ge.fit(x_train, y_train)\n",
    "\n",
    "pred = kde_ge.predict(x_train)\n",
    "\n",
    "print(classification_report(y_train, pred))\n",
    "y_test_1 = np.array([1 if s == 'AML' else 0 for s in y_train])\n",
    "pred_1 = np.array([1 if s == 'AML' else 0 for s in pred])\n",
    "fpr, tpr, _ = roc_curve(y_test_1, pred_1)\n",
    "plt.figure() \n",
    "plt.title('ROC')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.plot(fpr, tpr)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "E o desempenho obtido nos dados de teste:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kde_ge = KDE()\n",
    "kde_ge.fit(x_train, y_train)\n",
    "\n",
    "pred = kde_ge.predict(x_test)\n",
    "\n",
    "print(classification_report(y_test, pred))\n",
    "y_test_1 = np.array([1 if s == 'AML' else 0 for s in y_test])\n",
    "pred_1 = np.array([1 if s == 'AML' else 0 for s in pred])\n",
    "fpr, tpr, _ = roc_curve(y_test_1, pred_1)\n",
    "plt.figure() \n",
    "plt.title('ROC')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.plot(fpr, tpr)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A superfície de separação obtida pelo modelo pode ser vista plotada a seguir."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grid_size=100\n",
    "grid_range=(-100000, 100000)\n",
    "x_lab = np.linspace(grid_range[0], grid_range[1], num=grid_size)\n",
    "y_lab = np.linspace(grid_range[0], grid_range[1], num=grid_size)\n",
    "x1, x2 = np.meshgrid(x_lab, y_lab)\n",
    "x_grid = np.transpose(np.vstack([x1.flatten(), x2.flatten()]))\n",
    "\n",
    "y_hat = np.array(kde_ge.predict_indexes(x_grid))\n",
    "y_hat = y_hat.reshape([grid_size,grid_size])\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(x_test[y_test_1==0,0], x_test[y_test_1==0,1], '.', label='Classe 1')\n",
    "plt.plot(x_test[y_test_1==1,0], x_test[y_test_1==1,1], '.', label='Classe 2')\n",
    "plt.contour(x1, x2, y_hat, levels=[1], colors=('black',), linewidths=(2.5,))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}